{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scripts.db_utils import connect, sql_to_dataframe\n",
    "from src.data_quality_checks import check_missing_data, check_duplicates, check_data_types, check_numeric_anomalies, get_numeric_columns, get_total_missing_percentage\n",
    "from src.utils import bytes_to_gigabytes, kilobytes_per_second_to_megabytes_per_second, milliseconds_to_hours, milliseconds_to_minutes, bytes_to_megabytes, milliseconds_to_seconds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the connection\n",
    "conn = connect()\n",
    "\n",
    "query = \"\"\" SELECT * FROM public.xdr_data  \"\"\"\n",
    "\n",
    "#loading our dataframe\n",
    "df = sql_to_dataframe(conn, query)\n",
    "\n",
    "#closing the connection\n",
    "conn.close()\n",
    "\n",
    "# Letâ€™s see if we loaded the df successfully\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing value in each columns\n",
    "missing_data_summary = check_missing_data(df)\n",
    "print(missing_data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total percentage of missing values\n",
    "missing_data_percentage = get_total_missing_percentage(df)\n",
    "print(f\"Total Percentage of Missing Values: {missing_data_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicated rows in the datasets\n",
    "duplicate_rows = check_duplicates(df)\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for data type issues per each columns\n",
    "dtypes_summary = check_data_types(df)\n",
    "print(dtypes_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all numberical columns \n",
    "numeric_columns = get_numeric_columns(df)\n",
    "print(numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for anomalies in all numeric columns\n",
    "for numeric_column in numeric_columns:\n",
    "    numeric_anomalies = check_numeric_anomalies(df, numeric_column, lower_bound=0, upper_bound=None)\n",
    "    print(numeric_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total percentage of missing values\n",
    "missing_data_percentage = get_total_missing_percentage(df)\n",
    "print(f\"Total Percentage of Missing Values before cleaning: {missing_data_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical columns\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_columns = df.select_dtypes(exclude=[np.number]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numerical columns to the correct data type\n",
    "for col in numerical_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for numerical columns using mean strategy\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "df[numerical_columns] = num_imputer.fit_transform(df[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for categorical columns using most frequent strategy (mode)\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_columns] = cat_imputer.fit_transform(df[categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total percentage of missing values\n",
    "missing_data_percentage = get_total_missing_percentage(df)\n",
    "print(f\"Total Percentage of Missing Values after cleaning: {missing_data_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizing some columns and do some unit conversions \n",
    "df['Social Media (GB)'] = df['Social Media DL (Bytes)'].apply(bytes_to_megabytes) + df['Social Media UL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Youtube (GB)'] = df['Youtube DL (Bytes)'].apply(bytes_to_megabytes) + df['Youtube UL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Google (GB)'] = (df['Google DL (Bytes)'] + df['Google UL (Bytes)']).apply(bytes_to_megabytes)\n",
    "df['Email (GB)'] = (df['Email DL (Bytes)'] + df['Email UL (Bytes)']).apply(bytes_to_megabytes)\n",
    "df['Netflix (GB)'] = (df['Netflix DL (Bytes)'] + df['Netflix UL (Bytes)']).apply(bytes_to_megabytes)\n",
    "df['Gaming (GB)'] = (df['Gaming DL (Bytes)'] + df['Gaming UL (Bytes)']).apply(bytes_to_megabytes)\n",
    "df['Other (GB)'] = (df['Other DL (Bytes)'] + df['Other UL (Bytes)']).apply(bytes_to_megabytes)\n",
    "df['Total Data (GB)'] = df['Total DL (Bytes)'].apply(bytes_to_megabytes) + df['Total UL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Dur. (hr)'] = df['Dur. (ms).1'].apply(milliseconds_to_hours)\n",
    "df['Avg RTT DL (sec)'] = df['Avg RTT DL (ms)'].apply(milliseconds_to_seconds)\n",
    "df['Avg RTT UL (sec)'] = df['Avg RTT UL (ms)'].apply(milliseconds_to_seconds)\n",
    "df['Avg Bearer TP DL (Mbps)'] = df['Avg Bearer TP DL (kbps)'].apply(kilobytes_per_second_to_megabytes_per_second)\n",
    "df['Avg Bearer TP UL (Mbps)'] = df['Avg Bearer TP UL (kbps)'].apply(kilobytes_per_second_to_megabytes_per_second)\n",
    "\n",
    "# Apply conversion functions to columns and store results in new columns\n",
    "df['Total DL (Mb)'] = df['Total DL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Total UL (Mb)'] = df['Total UL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Social Media DL (Mb)'] = df['Social Media DL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Social Media UL (Mb)'] = df['Social Media UL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Google DL (Mb)'] = df['Google DL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Google UL (Mb)'] = df['Google UL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Email DL (Mb)'] = df['Email DL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Email UL (Mb)'] = df['Email UL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Youtube DL (Mb)'] = df['Youtube DL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Youtube UL (Mb)'] = df['Youtube UL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Netflix DL (Mb)'] = df['Netflix DL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Netflix UL (Mb)'] = df['Netflix UL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Gaming DL (Mb)'] = df['Gaming DL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Gaming UL (Mb)'] = df['Gaming UL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Other DL (Mb)'] = df['Other DL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Other UL (Mb)'] = df['Other UL (Bytes)'].apply(bytes_to_megabytes)\n",
    "df['Dur. (hr)'] = df['Dur. (ms).1'].apply(milliseconds_to_hours)\n",
    "df['Dur. (sec)'] = df['Dur. (ms).1'].apply(milliseconds_to_seconds)\n",
    "\n",
    "# Calculate total data volume (DL+UL) for each application\n",
    "df['Social Media Data (Mb)'] = df['Social Media DL (Mb)'] + df['Social Media UL (Mb)']\n",
    "df['Youtube Data (Mb)'] = df['Youtube DL (Mb)'] + df['Youtube UL (Mb)']\n",
    "df['Email Data (Mb)'] = df['Email DL (Mb)'] + df['Email UL (Mb)']\n",
    "df['Gaming Data (Mb)'] = df['Gaming DL (Mb)'] + df['Gaming UL (Mb)']\n",
    "df['Netflix Data (Mb)'] = df['Netflix DL (Mb)'] + df['Netflix UL (Mb)']\n",
    "df['Google Data (Mb)'] = df['Google DL (Mb)'] + df['Google UL (Mb)']\n",
    "df['Other Data (Mb)'] = df['Other DL (Mb)'] + df['Other UL (Mb)']\n",
    "df['Total Data (Mb)'] = df['Total DL (Mb)'] + df['Total UL (Mb)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with selected columns\n",
    "selected_columns = ['Total Data (Mb)', 'IMSI', 'Start',\n",
    "                    'Handset Manufacturer', 'Handset Type',\n",
    "                    'Avg Bearer TP UL (Mbps)', 'Avg Bearer TP DL (Mbps)', 'Avg RTT DL (sec)', 'Avg RTT UL (sec)',\n",
    "                    'Dur. (hr)',\n",
    "                    'Last Location Name',\n",
    "                    'Gaming Data (Mb)', 'Netflix Data (Mb)', 'Email Data (Mb)', 'Google Data (Mb)', 'Youtube Data (Mb)', 'Social Media Data (Mb)', 'Other Data (Mb)']\n",
    "\n",
    "df_relevant = df[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Tabulation for Handset Manufacturer\n",
    "print(\"--- Simple Tabulation for Handset Manufacturer ---\")\n",
    "counts_manufacturer = df_relevant['Handset Manufacturer'].value_counts()\n",
    "total_manufacturer = counts_manufacturer.sum()\n",
    "percentages_manufacturer = counts_manufacturer.apply(lambda x: round((x / total_manufacturer) * 100, 2))\n",
    "\n",
    "result_manufacturer = pd.DataFrame({'Handset Manufacturer': counts_manufacturer.index, 'Count': counts_manufacturer.values, 'Percentage': percentages_manufacturer.values})\n",
    "result_manufacturer.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Tabulation for Handset Type\n",
    "print(\"--- Simple Tabulation for Handset Type ---\")\n",
    "counts_type = df_relevant['Handset Type'].value_counts()\n",
    "total_type = counts_type.sum()\n",
    "percentages_type = counts_type.apply(lambda x: round((x / total_type) * 100, 2))\n",
    "\n",
    "result_type = pd.DataFrame({'Handset Type': counts_type.index, 'Count': counts_type.values, 'Percentage': percentages_type.values})\n",
    "top_10_result = result_type.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by count in ascending order\n",
    "top_10_result = top_10_result.sort_values(by='Count', ascending=True)\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_10_result['Handset Type'], top_10_result['Count'], color='deepblue')\n",
    "plt.title('Top 10 Handset Types (Count)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Tabulation for Last Location Name\n",
    "print(\"--- Simple Tabulation for Last Location Name ---\")\n",
    "counts_location = df_relevant['Last Location Name'].value_counts()\n",
    "total_location = counts_location.sum()\n",
    "percentages_location = counts_location.apply(lambda x: round((x / total_location) * 100, 2))\n",
    "\n",
    "result_location = pd.DataFrame({'Last Location Name': counts_location.index, 'Count': counts_location.values, 'Percentage': percentages_location.values})\n",
    "\n",
    "result_location.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to datetime format for accurate comparison\n",
    "date_column = pd.to_datetime(df['Start'])\n",
    "\n",
    "# Find the minimum and maximum dates\n",
    "min_date = date_column.min()\n",
    "max_date = date_column.max()\n",
    "\n",
    "print(\"Minimum Date:\", min_date)\n",
    "print(\"Maximum Date:\", max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list top 5 handset types for a given manufacturer\n",
    "def top_handsets_for_manufacturer(manufacturer):\n",
    "    df_manufacturer = df_relevant[df_relevant['Handset Manufacturer'] == manufacturer]\n",
    "    top_handsets = df_manufacturer['Handset Type'].value_counts().head(5)\n",
    "    top_handsets_percentage = (top_handsets / top_handsets.sum()) * 100\n",
    "    result_top_handsets = pd.DataFrame({'Handset Type': top_handsets.index, 'Count': top_handsets.values, 'Percentage': top_handsets_percentage.values})\n",
    "    return result_top_handsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manufacturer = 'Samsung'  # Enter the top manufacturer here\n",
    "top_handsets_df = top_handsets_for_manufacturer(manufacturer)\n",
    "samsung_handsets = top_handsets_df.head(5)\n",
    "samsung_handsets_count = samsung_handsets['Count']\n",
    "samsung_handsets_type = samsung_handsets['Handset Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by count in ascending order\n",
    "samsung_top_5_result = samsung_handsets.sort_values(by='Count', ascending=True)\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.barh(samsung_top_5_result['Handset Type'], samsung_top_5_result['Count'], color='green')\n",
    "plt.title('Top 5 Handset Types of Samsung')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manufacturer = 'Apple'  # Enter the top manufacturer here\n",
    "top_handsets_df = top_handsets_for_manufacturer(manufacturer)\n",
    "apple_handsets = top_handsets_df.head(5)\n",
    "apple_handsets_count = apple_handsets['Count']\n",
    "apple_handsets_type = apple_handsets['Handset Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by count in ascending order\n",
    "apple_top_5_result = apple_handsets.sort_values(by='Count', ascending=True)\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.barh(apple_top_5_result['Handset Type'], apple_top_5_result['Count'], color='brown')\n",
    "plt.title('Top 5 Handset Types of Apple')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manufacturer = 'Huawei'  # Enter the top manufacturer here\n",
    "top_handsets_df = top_handsets_for_manufacturer(manufacturer)\n",
    "huawei_handsets = top_handsets_df.head(5)\n",
    "huawei_handsets_count = huawei_handsets['Count']\n",
    "huawei_handsets_type = huawei_handsets['Handset Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by count in ascending order\n",
    "huawei_top_5_result = huawei_handsets.sort_values(by='Count', ascending=True)\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.barh(huawei_top_5_result['Handset Type'], huawei_top_5_result['Count'], color='grey')\n",
    "plt.title('Top 5 Handset Types of Huawei')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables descriptive summary\n",
    "numerical_columns = ['Total Data (Mb)', 'Avg Bearer TP UL (Mbps)',\n",
    "                     'Avg Bearer TP DL (Mbps)', 'Avg RTT DL (sec)', 'Avg RTT UL (sec)', 'Dur. (hr)',\n",
    "                     'Gaming Data (Mb)', 'Netflix Data (Mb)', 'Email Data (Mb)', 'Google Data (Mb)', 'Youtube Data (Mb)', 'Social Media Data (Mb)', 'Other Data (Mb)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate descriptive statistics\n",
    "df_descriptions = df_relevant[numerical_columns].describe()\n",
    "df_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mode\n",
    "mode_values = df_relevant[numerical_columns].mode().iloc[0]\n",
    "mode_values.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate range\n",
    "range_values = df_relevant[numerical_columns].max() - df_relevant[numerical_columns].min()\n",
    "range_values.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate skewness\n",
    "skewness_values = df_relevant[numerical_columns].skew()\n",
    "skewness_values.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sum\n",
    "sum_values = df_relevant[numerical_columns].sum()\n",
    "sum_values.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating User Behavior Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by user\n",
    "grouped_data = df.groupby('IMSI')\n",
    "\n",
    "# Step 2: Compute aggregates\n",
    "user_behavior_summary = grouped_data.agg({\n",
    "    'Bearer Id': 'count',\n",
    "    'Dur. (hr)': 'sum',\n",
    "    'Total Data (Mb)': 'sum',\n",
    "    'Social Media Data (Mb)': 'sum',\n",
    "    'Google Data (Mb)': 'sum',\n",
    "    'Email Data (Mb)': 'sum',\n",
    "    'Youtube Data (Mb)': 'sum',\n",
    "    'Netflix Data (Mb)': 'sum',\n",
    "    'Gaming Data (Mb)': 'sum',\n",
    "    'Other Data (Mb)': 'sum',\n",
    "})\n",
    "\n",
    "user_behavior_summary.rename(columns={'Bearer Id': 'Number of Sessions'}, inplace=True)\n",
    "\n",
    "# Display the aggregated user behavior data\n",
    "user_behavior_summary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of users \n",
    "unique_rows = df.drop_duplicates(subset=['IMSI'])\n",
    "users_count = unique_rows['IMSI'].count()\n",
    "print(users_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Start' to datetime format using .loc to avoid SettingWithCopyWarning\n",
    "df_relevant.loc[:, 'Start'] = pd.to_datetime(df_relevant['Start'])\n",
    "\n",
    "# Extract date part only and save it to a new column 'Date' using .loc\n",
    "df_relevant.loc[:, 'Date'] = df_relevant['Start'].dt.date\n",
    "\n",
    "# Convert 'Date' to datetime format for plotting using .loc\n",
    "df_relevant.loc[:, 'Date'] = pd.to_datetime(df_relevant['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to exclude\n",
    "columns_to_exclude = ['Last Location Name', 'Handset Manufacturer', 'Handset Type', 'IMSI']\n",
    "\n",
    "# Create a new DataFrame by excluding the specified columns\n",
    "new_df = df_relevant.drop(columns=columns_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dates = df_relevant['Date'].drop_duplicates().reset_index(drop=True)\n",
    "unique_dates = pd.to_datetime(unique_dates.dropna())\n",
    "print(unique_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'Date' column exists in df_relevant\n",
    "if 'Date' in df_relevant.columns:\n",
    "    unique_dates = df_relevant['Date'].drop_duplicates().reset_index(drop=True)\n",
    "    unique_dates = pd.to_datetime(unique_dates.dropna())  # Exclude undefined values before conversion\n",
    "    \n",
    "    # Create a new DataFrame to store the results\n",
    "    new_df = pd.DataFrame(columns=list(df_relevant.columns))  # Include all columns\n",
    "    \n",
    "    # Iterate over unique dates and filter rows in df_relevant for each date\n",
    "    for date in unique_dates:\n",
    "        filtered_rows = df_relevant[df_relevant['Date'] == date]\n",
    "        filtered_rows = filtered_rows.drop(columns=['Date'])  # Drop 'Date' column\n",
    "        \n",
    "        # Select numeric columns only and calculate the sum\n",
    "        sum_values = filtered_rows.select_dtypes(include=[int, float]).sum()\n",
    "        \n",
    "        # Append the sum to the new DataFrame\n",
    "        new_row = [date] + sum_values.tolist()\n",
    "        \n",
    "        # Ensure new_row has the same length as the number of columns in new_df\n",
    "        while len(new_row) < len(new_df.columns):\n",
    "            new_row.append(None)  # Add None for missing columns\n",
    "        \n",
    "        new_df.loc[len(new_df)] = new_row\n",
    "    \n",
    "    new_df.head(30)\n",
    "else:\n",
    "    print(\"No 'Date' column found in df_relevant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_chart(x_values, y_values, x_label='X-axis', y_label='Y-axis', title='Bar Chart'):\n",
    "    \"\"\"\n",
    "    Plot a bar chart based on provided x and y values.\n",
    "\n",
    "    Args:\n",
    "    x_values (list or array-like): Values for the x-axis.\n",
    "    y_values (list or array-like): Values for the y-axis.\n",
    "    x_label (str): Label for the x-axis (default is 'X-axis').\n",
    "    y_label (str): Label for the y-axis (default is 'Y-axis').\n",
    "    title (str): Title of the chart (default is 'Bar Chart').\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "    plt.bar(x_values, y_values, width=0.5)  # Plotting the bar chart\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Start' to datetime format\n",
    "df_relevant['Start'] = pd.to_datetime(df_relevant['Start'])\n",
    "\n",
    "# Extract date part only and save it to a new column 'Date'\n",
    "df_relevant['Date'] = df_relevant['Start'].dt.date\n",
    "\n",
    "# Create a new DataFrame to store the aggregated data\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "print(df_relevant.columns)\n",
    "\n",
    "# Group by 'Date' and sum 'Total Data (Gb)' for each date\n",
    "grouped_data = df.groupby('Date')['Total Data (Mb)'].sum().reset_index()\n",
    "\n",
    "# Copy the 'Date' and aggregated 'Total DL (Bytes)' to the new DataFrame\n",
    "new_df['Date'] = grouped_data['Date']\n",
    "new_df['Total Data (Mb)'] = grouped_data['Total Data (Mb)']\n",
    "\n",
    "# Convert 'Total Data DL (Bytes)' to gigabytes\n",
    "# new_df['Total Data DL (Gb)'] = new_df['Total Data DL (Bytes)'].apply(bytes_to_gigabytes)\n",
    "\n",
    "# Plotting the chart with all date values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(new_df['Date'], new_df['Total Data (Mb)'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Data (Mb)')\n",
    "plt.title('Total Data (Mb) Variation by Date')\n",
    "plt.xticks(new_df['Date'], rotation=45)  # Set x-ticks to all date values and rotate labels\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "plt.grid(True)  # Add gridlines for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relevant.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
